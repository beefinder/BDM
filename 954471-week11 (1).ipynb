{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-27T03:46:43.676054Z","iopub.execute_input":"2021-09-27T03:46:43.676365Z","iopub.status.idle":"2021-09-27T03:46:43.795207Z","shell.execute_reply.started":"2021-09-27T03:46:43.676290Z","shell.execute_reply":"2021-09-27T03:46:43.794375Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\nimport matplotlib.pyplot as plt\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:43.796650Z","iopub.execute_input":"2021-09-27T03:46:43.797029Z","iopub.status.idle":"2021-09-27T03:46:45.363598Z","shell.execute_reply.started":"2021-09-27T03:46:43.796984Z","shell.execute_reply":"2021-09-27T03:46:45.362808Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import glob\nimgdir = \"/kaggle/input/954471-bp/\"\nimg_names = sorted(glob.glob(imgdir + \"*.jpg\"))\nlabel_names = sorted(glob.glob(imgdir+ \"*.txt\"))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:45.366786Z","iopub.execute_input":"2021-09-27T03:46:45.367057Z","iopub.status.idle":"2021-09-27T03:46:45.375051Z","shell.execute_reply.started":"2021-09-27T03:46:45.367024Z","shell.execute_reply":"2021-09-27T03:46:45.374245Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"img_names","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:45.377188Z","iopub.execute_input":"2021-09-27T03:46:45.378183Z","iopub.status.idle":"2021-09-27T03:46:45.387157Z","shell.execute_reply.started":"2021-09-27T03:46:45.378154Z","shell.execute_reply":"2021-09-27T03:46:45.386268Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"BOX_COLOR = (255, 0, 0) # Red\nTEXT_COLOR = (255, 255, 255) # White\ndef read_label(labelfile):\n    content = open(labelfile).readlines()\n    list_with_all_boxes = []\n    list_names = []\n    for item in content:\n        classname, xcen, ycen, w, h = item.rstrip().split()\n        list_with_single_boxes = [float(xcen),float(ycen),float(w),float(h)]\n        list_with_all_boxes.append(list_with_single_boxes)\n        list_names.append(int(classname))\n    return list_names, list_with_all_boxes\n\ndef visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n    \"\"\"Visualizes a single bounding box on the image\"\"\"\n    imgSize = np.shape(img)\n    xcen, ycen, w, h = bbox\n    x_min = int(max(xcen - w/2, 0)*imgSize[1])\n    x_max = int(min(xcen + w/2, 1)*imgSize[1])\n    y_min = int(max(ycen - h/2, 0)*imgSize[0])\n    y_max = int(min(ycen + h/2, 1)*imgSize[0])\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n\n    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n    cv2.putText(\n        img,\n        text=class_name,\n        org=(x_min, y_min - int(0.3 * text_height)),\n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=0.35, \n        color=TEXT_COLOR, \n        lineType=cv2.LINE_AA,\n    )\n    return img\n\n\ndef visualize(image, bboxes, category_ids, category_id_to_name):\n    img = image.copy()\n    for bbox, category_id in zip(bboxes, category_ids):\n        class_name = category_id_to_name[category_id]\n        img = visualize_bbox(img, bbox, class_name)\n    plt.figure(figsize=(12, 12))\n    plt.axis('off')\n    plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:45.388444Z","iopub.execute_input":"2021-09-27T03:46:45.388830Z","iopub.status.idle":"2021-09-27T03:46:45.405583Z","shell.execute_reply.started":"2021-09-27T03:46:45.388796Z","shell.execute_reply":"2021-09-27T03:46:45.404805Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\ncategory_id_to_name = {0: \"Rose\"}\nimage = np.array(Image.open(img_names[5]))\nclass_labels, bboxes = read_label(label_names[5])\nvisualize(image, bboxes, class_labels, category_id_to_name)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:45.407113Z","iopub.execute_input":"2021-09-27T03:46:45.407408Z","iopub.status.idle":"2021-09-27T03:46:45.886412Z","shell.execute_reply.started":"2021-09-27T03:46:45.407372Z","shell.execute_reply":"2021-09-27T03:46:45.885722Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Function for Albumentation (Augamentation )\nimg_size = 416\ntrain_transform = A.Compose([\n    A.Resize(width=img_size, height=img_size), #resize to square size\n    A.HorizontalFlip(p=0.5), #random horizontal flip \n    A.VerticalFlip(p=0.5), #random verizontal flip \n    A.ShiftScaleRotate(shift_limit=0.25, scale_limit=0.25, rotate_limit=45, p=0.5), #บิดและย่อขยาย 0.25 หมุนได้ 45 องศา ความน่าจะเป็น 0.5\n    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5), #color\n    A.RandomBrightnessContrast(p=0.5), #brighth\n    A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.3), #color \n], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'], min_visibility=0.2))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:45.887385Z","iopub.execute_input":"2021-09-27T03:46:45.887616Z","iopub.status.idle":"2021-09-27T03:46:45.896404Z","shell.execute_reply.started":"2021-09-27T03:46:45.887587Z","shell.execute_reply":"2021-09-27T03:46:45.895362Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#try train_transform function\ntransformed = train_transform(image=image, bboxes=bboxes, class_labels=class_labels)\ntransformed_image = transformed['image']\ntransformed_bboxes = transformed['bboxes']\ntransformed_class_labels = transformed['class_labels']\nvisualize(transformed_image, transformed_bboxes, transformed_class_labels, category_id_to_name) #show","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:45.898103Z","iopub.execute_input":"2021-09-27T03:46:45.898500Z","iopub.status.idle":"2021-09-27T03:46:46.436152Z","shell.execute_reply.started":"2021-09-27T03:46:45.898444Z","shell.execute_reply":"2021-09-27T03:46:46.435366Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!mkdir ROSE #directory for keep image that already transform (train images)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:46.437490Z","iopub.execute_input":"2021-09-27T03:46:46.437737Z","iopub.status.idle":"2021-09-27T03:46:47.230962Z","shell.execute_reply.started":"2021-09-27T03:46:46.437706Z","shell.execute_reply":"2021-09-27T03:46:47.230019Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#keep image that already transform (train images) to ROSE directory \noutdir = \"./ROSE/\"\nfor i in range(len(img_names)-8): #last 5 images spilt to be validation data\n    image = np.array(Image.open(img_names[i]))\n    class_labels, bboxes = read_label(label_names[i])\n    onlyname = img_names[i].split('/')[-1].split('.')[0]\n    for ii in range(20): #for 1 images -> agamentation 20 : 12 train images -> 240 train images\n        transformed = train_transform(image=image, bboxes=bboxes, class_labels=class_labels)\n        transformed_image = transformed['image']\n        transformed_bboxes = transformed['bboxes']\n        transformed_class_labels = transformed['class_labels']\n        transformed_name = onlyname+'_'+str(ii)\n        cv2.imwrite(outdir + transformed_name +'.jpg', cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)) \n        out_file = open(outdir + transformed_name +'.txt', 'w')\n        for iii in range(len(transformed_bboxes)):\n            bIn  =  A.augmentations.bbox_utils.convert_bbox_to_albumentations(transformed_bboxes[iii], 'yolo', img_size, img_size, check_validity=True)\n            bOut = A.augmentations.bbox_utils.convert_bbox_from_albumentations(bIn, 'yolo', img_size, img_size, check_validity=True)\n            out_file.write(str(0) + \" \" + \" \".join([str(b) for b in bOut]) + '\\n')\n        out_file.close()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:47.237714Z","iopub.execute_input":"2021-09-27T03:46:47.239830Z","iopub.status.idle":"2021-09-27T03:46:49.962434Z","shell.execute_reply.started":"2021-09-27T03:46:47.239788Z","shell.execute_reply":"2021-09-27T03:46:49.961617Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!ls ./ROSE/* | wc -l #numbers of train images = 480 (images 240 + labels 240)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:49.963724Z","iopub.execute_input":"2021-09-27T03:46:49.964191Z","iopub.status.idle":"2021-09-27T03:46:50.631283Z","shell.execute_reply.started":"2021-09-27T03:46:49.964144Z","shell.execute_reply":"2021-09-27T03:46:50.630453Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!mkdir ROSE_VAL #directory for keep validation image ","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:50.634311Z","iopub.execute_input":"2021-09-27T03:46:50.634548Z","iopub.status.idle":"2021-09-27T03:46:51.308161Z","shell.execute_reply.started":"2021-09-27T03:46:50.634507Z","shell.execute_reply":"2021-09-27T03:46:51.307163Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Augumentation validation images \nval_transform = A.Compose([\n    A.Resize(width=img_size, height=img_size), #just resize to validation in model\n], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'], min_visibility=0.2))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:51.309511Z","iopub.execute_input":"2021-09-27T03:46:51.309771Z","iopub.status.idle":"2021-09-27T03:46:51.316800Z","shell.execute_reply.started":"2021-09-27T03:46:51.309734Z","shell.execute_reply":"2021-09-27T03:46:51.316028Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#keep validation image to ROSE_VAL directory \noutdir = \"./ROSE_VAL/\"\nfor i in range(12, len(img_names)-2): #validation 6 images / Test 2 images\n    image = cv2.imread(img_names[i])\n    class_labels, bboxes = read_label(label_names[i])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    onlyname = img_names[i].split('/')[-1].split('.')[0]\n\n    transformed = val_transform(image=image, bboxes=bboxes, class_labels=class_labels)\n    transformed_image = transformed['image']\n    transformed_bboxes = transformed['bboxes']\n    transformed_class_labels = transformed['class_labels']\n    transformed_name = onlyname\n    cv2.imwrite(outdir + transformed_name +'.jpg', cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)) \n    out_file = open(outdir + transformed_name +'.txt', 'w')\n    for iii in range(len(transformed_bboxes)):\n        bIn  =  A.augmentations.bbox_utils.convert_bbox_to_albumentations(transformed_bboxes[iii], 'yolo', img_size, img_size, check_validity=True)\n        bOut = A.augmentations.bbox_utils.convert_bbox_from_albumentations(bIn, 'yolo', img_size, img_size, check_validity=True)\n        out_file.write(str(0) + \" \" + \" \".join([str(b) for b in bOut]) + '\\n')\n    out_file.close()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:51.319338Z","iopub.execute_input":"2021-09-27T03:46:51.320769Z","iopub.status.idle":"2021-09-27T03:46:51.569630Z","shell.execute_reply.started":"2021-09-27T03:46:51.320298Z","shell.execute_reply":"2021-09-27T03:46:51.568794Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!ls ./ROSE_VAL/* | wc -l #(images 6 + labels 6)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:51.573587Z","iopub.execute_input":"2021-09-27T03:46:51.573835Z","iopub.status.idle":"2021-09-27T03:46:52.232137Z","shell.execute_reply.started":"2021-09-27T03:46:51.573803Z","shell.execute_reply":"2021-09-27T03:46:52.231291Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#import YOLO by pythoch (Framework)\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nprint(torch.__version__, torchvision.__version__)\nos.getcwd()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:52.235463Z","iopub.execute_input":"2021-09-27T03:46:52.235692Z","iopub.status.idle":"2021-09-27T03:46:56.682359Z","shell.execute_reply.started":"2021-09-27T03:46:52.235665Z","shell.execute_reply":"2021-09-27T03:46:56.681689Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5.git #git clone command for download program","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:56.683527Z","iopub.execute_input":"2021-09-27T03:46:56.683770Z","iopub.status.idle":"2021-09-27T03:46:58.756601Z","shell.execute_reply.started":"2021-09-27T03:46:56.683739Z","shell.execute_reply":"2021-09-27T03:46:58.755660Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/yolov5\") #run at yolov5 directory\n!pip install -r ./requirements.txt","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:46:58.759989Z","iopub.execute_input":"2021-09-27T03:46:58.760235Z","iopub.status.idle":"2021-09-27T03:47:06.838353Z","shell.execute_reply.started":"2021-09-27T03:46:58.760204Z","shell.execute_reply":"2021-09-27T03:47:06.837529Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/yolov5\") #run at yolov5 directory\n#import train images from ROSE directory to train.txt\nimport glob\nfilename = glob.glob('/kaggle/working/ROSE/*.jpg')\nfilename.sort()\nf = open(\"./train.txt\", \"w\")\nfor name in filename:\n    f.write(name+\"\\n\")\nf.close()\n\n#import validation images from ROSE_VAL directory to val.txt\nfilename = glob.glob('/kaggle/working/ROSE_VAL/*.jpg')\nfilename.sort()\nf = open(\"./val.txt\", \"w\")\nfor name in filename:\n    f.write(name+\"\\n\")\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:47:06.839880Z","iopub.execute_input":"2021-09-27T03:47:06.840180Z","iopub.status.idle":"2021-09-27T03:47:06.851143Z","shell.execute_reply.started":"2021-09-27T03:47:06.840141Z","shell.execute_reply":"2021-09-27T03:47:06.850368Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#create train directory\nos.chdir(\"/kaggle/working/yolov5\")\nf = open(\"./train.yaml\", \"w\")\nf.write('train: ./train.txt'+\"\\n\")\nf.write('val: ./val.txt'+\"\\n\")\nf.write('nc: 1'+\"\\n\")\nf.write('names: [ \\'ROSE\\' ]'+\"\\n\")\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:47:06.852691Z","iopub.execute_input":"2021-09-27T03:47:06.852999Z","iopub.status.idle":"2021-09-27T03:47:06.860426Z","shell.execute_reply.started":"2021-09-27T03:47:06.852967Z","shell.execute_reply":"2021-09-27T03:47:06.859665Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"!ls ./yolov5/*","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:47:06.863283Z","iopub.execute_input":"2021-09-27T03:47:06.863681Z","iopub.status.idle":"2021-09-27T03:47:07.509598Z","shell.execute_reply.started":"2021-09-27T03:47:06.863654Z","shell.execute_reply":"2021-09-27T03:47:07.508764Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"!wandb disabled","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:47:07.511010Z","iopub.execute_input":"2021-09-27T03:47:07.511277Z","iopub.status.idle":"2021-09-27T03:47:09.497455Z","shell.execute_reply.started":"2021-09-27T03:47:07.511242Z","shell.execute_reply":"2021-09-27T03:47:09.496618Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# **Train**","metadata":{}},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/yolov5\")\n!python train.py --img 416 --batch 8 --epochs 100 --data train.yaml --cfg ./models/yolov5s.yaml --name ROSERUN #keep result at ROSERUN directory","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:47:09.500819Z","iopub.execute_input":"2021-09-27T03:47:09.501059Z","iopub.status.idle":"2021-09-27T03:57:51.778825Z","shell.execute_reply.started":"2021-09-27T03:47:09.501031Z","shell.execute_reply":"2021-09-27T03:57:51.778042Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"<p> mAP@0.5 = mean average precision เมื่อสนใจ IOU >50% ขึ้นไป \n<p> mAP@ = คำนวณความแม่นที่มีค่า confident สูงเกิน 50% ขึ้นไป","metadata":{}},{"cell_type":"code","source":"#Train rate Graph \nos.chdir(\"/kaggle/working/yolov5\")\nfrom utils.plots import plot_results \nplot_results('runs/train/ROSERUN/results.csv')\nimage = np.array(Image.open('runs/train/ROSERUN/results.png'))\nplt.figure(figsize=(20, 20))\nplt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:58.625667Z","iopub.execute_input":"2021-09-27T03:57:58.625943Z","iopub.status.idle":"2021-09-27T03:58:00.148594Z","shell.execute_reply.started":"2021-09-27T03:57:58.625894Z","shell.execute_reply":"2021-09-27T03:58:00.147833Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/yolov5\")\nfrom models.experimental import attempt_load\nfrom utils.torch_utils import select_device\nweights = './runs/train/ROSERUN/weights/best.pt' #select best model\ndevice = select_device('cpu')\nmodel = attempt_load(weights, map_location=device)  # load FP32 model\nstride = int(model.stride.max())  # model stride\nnames = model.module.names if hasattr(model, 'module') else model.names  # get class names","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:53.906966Z","iopub.execute_input":"2021-09-27T03:57:53.907246Z","iopub.status.idle":"2021-09-27T03:57:54.209258Z","shell.execute_reply.started":"2021-09-27T03:57:53.907212Z","shell.execute_reply":"2021-09-27T03:57:54.208401Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/ROSE_VAL/*","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:54.210589Z","iopub.execute_input":"2021-09-27T03:57:54.210837Z","iopub.status.idle":"2021-09-27T03:57:54.870137Z","shell.execute_reply.started":"2021-09-27T03:57:54.210804Z","shell.execute_reply":"2021-09-27T03:57:54.869175Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#code for keep precision result \nos.chdir(\"/kaggle/working/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('/kaggle/working/ROSE_VAL/blackpink_2.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img / 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) / 2 / w;\n                ycen = (ymin + ymax) / 2 / h;\n                ww = (xmax - xmin) / w;\n                hh = (ymax - ymin) / h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:54.873274Z","iopub.execute_input":"2021-09-27T03:57:54.873516Z","iopub.status.idle":"2021-09-27T03:57:55.069454Z","shell.execute_reply.started":"2021-09-27T03:57:54.873480Z","shell.execute_reply":"2021-09-27T03:57:55.068690Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:55.073859Z","iopub.execute_input":"2021-09-27T03:57:55.074089Z","iopub.status.idle":"2021-09-27T03:57:55.521385Z","shell.execute_reply.started":"2021-09-27T03:57:55.074065Z","shell.execute_reply":"2021-09-27T03:57:55.515355Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"confs","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:55.522754Z","iopub.execute_input":"2021-09-27T03:57:55.523008Z","iopub.status.idle":"2021-09-27T03:57:55.529102Z","shell.execute_reply.started":"2021-09-27T03:57:55.522977Z","shell.execute_reply":"2021-09-27T03:57:55.528156Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#code for keep precision result \nos.chdir(\"/kaggle/working/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('/kaggle/working/ROSE_VAL/blackpink_3.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img / 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) / 2 / w;\n                ycen = (ymin + ymax) / 2 / h;\n                ww = (xmax - xmin) / w;\n                hh = (ymax - ymin) / h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:55.530414Z","iopub.execute_input":"2021-09-27T03:57:55.530772Z","iopub.status.idle":"2021-09-27T03:57:55.692040Z","shell.execute_reply.started":"2021-09-27T03:57:55.530743Z","shell.execute_reply":"2021-09-27T03:57:55.691239Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:55.693474Z","iopub.execute_input":"2021-09-27T03:57:55.693747Z","iopub.status.idle":"2021-09-27T03:57:56.127600Z","shell.execute_reply.started":"2021-09-27T03:57:55.693712Z","shell.execute_reply":"2021-09-27T03:57:56.126810Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"confs","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:56.128867Z","iopub.execute_input":"2021-09-27T03:57:56.129236Z","iopub.status.idle":"2021-09-27T03:57:56.134954Z","shell.execute_reply.started":"2021-09-27T03:57:56.129198Z","shell.execute_reply":"2021-09-27T03:57:56.134207Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#code for keep precision result \nos.chdir(\"/kaggle/working/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('/kaggle/working/ROSE_VAL/blackpink_4.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img / 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) / 2 / w;\n                ycen = (ymin + ymax) / 2 / h;\n                ww = (xmax - xmin) / w;\n                hh = (ymax - ymin) / h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:56.136383Z","iopub.execute_input":"2021-09-27T03:57:56.136860Z","iopub.status.idle":"2021-09-27T03:57:56.302406Z","shell.execute_reply.started":"2021-09-27T03:57:56.136826Z","shell.execute_reply":"2021-09-27T03:57:56.301636Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:56.303606Z","iopub.execute_input":"2021-09-27T03:57:56.304205Z","iopub.status.idle":"2021-09-27T03:57:56.755662Z","shell.execute_reply.started":"2021-09-27T03:57:56.304172Z","shell.execute_reply":"2021-09-27T03:57:56.754812Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"confs","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:56.757072Z","iopub.execute_input":"2021-09-27T03:57:56.757315Z","iopub.status.idle":"2021-09-27T03:57:56.762812Z","shell.execute_reply.started":"2021-09-27T03:57:56.757286Z","shell.execute_reply":"2021-09-27T03:57:56.762026Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"#code for keep precision result \nos.chdir(\"/kaggle/working/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('/kaggle/working/ROSE_VAL/blackpink_5.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img / 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) / 2 / w;\n                ycen = (ymin + ymax) / 2 / h;\n                ww = (xmax - xmin) / w;\n                hh = (ymax - ymin) / h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:56.764255Z","iopub.execute_input":"2021-09-27T03:57:56.764517Z","iopub.status.idle":"2021-09-27T03:57:56.920122Z","shell.execute_reply.started":"2021-09-27T03:57:56.764487Z","shell.execute_reply":"2021-09-27T03:57:56.919346Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:56.921303Z","iopub.execute_input":"2021-09-27T03:57:56.921928Z","iopub.status.idle":"2021-09-27T03:57:57.309619Z","shell.execute_reply.started":"2021-09-27T03:57:56.921873Z","shell.execute_reply":"2021-09-27T03:57:57.308801Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"confs","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:57.310987Z","iopub.execute_input":"2021-09-27T03:57:57.311269Z","iopub.status.idle":"2021-09-27T03:57:57.317085Z","shell.execute_reply.started":"2021-09-27T03:57:57.311231Z","shell.execute_reply":"2021-09-27T03:57:57.316276Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#code for keep precision result \nos.chdir(\"/kaggle/working/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('/kaggle/working/ROSE_VAL/blackpink_6.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img / 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) / 2 / w;\n                ycen = (ymin + ymax) / 2 / h;\n                ww = (xmax - xmin) / w;\n                hh = (ymax - ymin) / h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:57.318655Z","iopub.execute_input":"2021-09-27T03:57:57.318985Z","iopub.status.idle":"2021-09-27T03:57:57.473783Z","shell.execute_reply.started":"2021-09-27T03:57:57.318950Z","shell.execute_reply":"2021-09-27T03:57:57.473046Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:57.475104Z","iopub.execute_input":"2021-09-27T03:57:57.475509Z","iopub.status.idle":"2021-09-27T03:57:57.936744Z","shell.execute_reply.started":"2021-09-27T03:57:57.475473Z","shell.execute_reply":"2021-09-27T03:57:57.935830Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"confs","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:57.938355Z","iopub.execute_input":"2021-09-27T03:57:57.938633Z","iopub.status.idle":"2021-09-27T03:57:57.944979Z","shell.execute_reply.started":"2021-09-27T03:57:57.938597Z","shell.execute_reply":"2021-09-27T03:57:57.944113Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"#code for keep precision result \nos.chdir(\"/kaggle/working/yolov5\")\nfrom utils.datasets import LoadImages\nfrom utils.general import check_img_size, non_max_suppression, scale_coords\nfrom utils.plots import Annotator, colors\nh = 416\nw = 416\nimgsz = check_img_size([h,w], s=stride) \ndataset = LoadImages('/kaggle/working/ROSE_VAL/blackpink_7.jpg', img_size=imgsz, stride=stride, auto=True)\n\npred_results = []\nfor path, img, im0s, vid_cap in dataset:\n    img = torch.from_numpy(img).to(device)\n    img = img / 255.0\n    if len(img.shape) == 3:\n        img = img[None]\n    pred = model(img)[0]\n    pred = non_max_suppression(pred, 0.1, 0.45, None, False, max_det=1000)\n    bboxes = []\n    labels = []\n    confs = []\n    for i, det in enumerate(pred):\n        if len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n            for *xyxy, conf, cls in reversed(det):\n                labels.append(int(cls))\n                confs.append(conf.item())\n                xmin, ymin, xmax, ymax = xyxy\n                xcen = (xmin + xmax) / 2 / w;\n                ycen = (ymin + ymax) / 2 / h;\n                ww = (xmax - xmin) / w;\n                hh = (ymax - ymin) / h;\n                bboxes.append([xcen.item(), ycen.item(), ww.item(), hh.item()])\n    pred_results.append([path, bboxes, labels, confs])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:57.947089Z","iopub.execute_input":"2021-09-27T03:57:57.947396Z","iopub.status.idle":"2021-09-27T03:57:58.136846Z","shell.execute_reply.started":"2021-09-27T03:57:57.947361Z","shell.execute_reply":"2021-09-27T03:57:58.135106Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"path, bboxes, labels, confs = pred_results[0]\nimage = np.array(Image.open(path))\nvisualize(image, bboxes, labels, names) #show","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:58.138070Z","iopub.execute_input":"2021-09-27T03:57:58.138397Z","iopub.status.idle":"2021-09-27T03:57:58.616616Z","shell.execute_reply.started":"2021-09-27T03:57:58.138356Z","shell.execute_reply":"2021-09-27T03:57:58.615806Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"confs","metadata":{"execution":{"iopub.status.busy":"2021-09-27T03:57:58.617837Z","iopub.execute_input":"2021-09-27T03:57:58.618167Z","iopub.status.idle":"2021-09-27T03:57:58.624313Z","shell.execute_reply.started":"2021-09-27T03:57:58.618134Z","shell.execute_reply":"2021-09-27T03:57:58.623517Z"},"trusted":true},"execution_count":44,"outputs":[]}]}